{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.L1Loss,绝对值误差，MAE\n",
    "1. 计算公式\n",
    "$$loss(\\hat{y},y) = \\left \\| \\hat{y} - y \\right \\|$$\n",
    "2. torch参数\n",
    "    ```python\n",
    "    torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "    ```\n",
    "3. 参数解释\n",
    "    + reduce，没有规约\n",
    "    $$loss(\\hat{y},y) = L = \\left \\{ L_{1},..., L_{m} \\right \\}$$\n",
    "    + reduce，规约\n",
    "    $$ loss(\\hat{y},y) = \\left \\{ \\begin{matrix}mean(L),reduction='mean' \\\\ sum(L),reudciton='sum'\\end{matrix}\\right.$$\n",
    "    + 默认情况下\n",
    "    $$reduce=True,size\\_average=True,reduction='mean'$$ \n",
    "4. 测试代码\n",
    "```python\n",
    "# 测试代码\n",
    "y_hat = torch.ones(10,1)\n",
    "noise =  torch.randn(10,1)\n",
    "y = torch.ones(10,1)\n",
    "loss = nn.L1Loss()\n",
    "print(loss(y_hat,y),noise)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.SmoothL1Loss\n",
    "1. 解释：\n",
    "    + 也被称为Huber Loss\n",
    "    + 当预测偏差小于 a 时，它采用平方误差,\n",
    "    + 当预测偏差大于 a 时，采用的线性误差\n",
    "    + a代表残差$a = \\|y-\\hat{y}\\|$\n",
    "2. 公式,L1loss代表$\\delta=1$\n",
    "$$\\begin{split}L_\\delta(a)=\\left \\{\\begin{array}{ll}\\frac12a^2,&\\textrm{for } |a|\\leq\\delta,\\\\\\delta\\cdot(|a|-\\frac12\\delta),&\\textrm{otherwise.}\\end{array}\\right.\\end{split}$$\n",
    "3. 代码\n",
    "```python\n",
    "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "4. 可视化Huber Loss\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "def Phi(t, c):\n",
    "    t = abs(t)\n",
    "    flag = (t > c)\n",
    "    return (~flag) * (0.5 * t ** 2) - (flag) * c * (0.5 * c - t)\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "ax = fig.add_subplot(111)\n",
    "# x被定义为残差 \n",
    "x = np.linspace(-10, 10, 100)\n",
    "for c in (20, 2, 3, 5, 1000):\n",
    "    y = Phi(x, c)\n",
    "    ax.plot(x, y, '-k')\n",
    "    ax.hold\n",
    "    ax.plot(x, 0.5*x*x, '-b')\n",
    "    if c > 10:\n",
    "        s = r'\\infty'\n",
    "    else:\n",
    "        s = str(c)\n",
    "    ax.text(x[6], y[6], '$c=%s$' % s,\n",
    "            ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel(r'$\\Phi(t)$')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.MSELoss() 二次代价函数\n",
    "1. 公式\n",
    "$$\\text{loss}(\\mathbf{x}_i, \\mathbf{y}_i) = (\\mathbf{x}_i - \\mathbf{y}_i)^2$$\n",
    "2. 用法和MAE相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.BCELoss  二元交叉熵损失函数\n",
    "1. 解释\n",
    "    + 用来处理二分类问题，注意$y$的数值应当在0到1之间，需要利用sigmoid函数处理,默认是$mean$\n",
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],$$\n",
    "        $$       \\ell(x, y) = \\begin{cases}\n",
    "            \\operatorname{mean}(L), & \\text{if reduction} = \\text{'mean';}\\\\\n",
    "            \\operatorname{sum}(L),  & \\text{if reduction} = \\text{'sum'.}\n",
    "        \\end{cases}$$\n",
    "2. 定义一下对数损失函数，对于二分类问题，类标签定义为：\n",
    "$$y_{i} = \\{0,1\\}$$\n",
    "计算得到样本为$1$的概率为$p$，由于是二分类问题，那么样本为$0$的概率为$1-p$,将两者合并在一起，可以表示为：\n",
    "$$p(Y|X,\\theta) =p^{y}(1-p)^{1-y} = \\left\\{\\begin{matrix}p, y=1\\\\ 1-p,y=0\\end{matrix}\\right.$$\n",
    "对于一个样本可以理解为，\n",
    "    + 当类标签为1时，希望样本为$1$的概率$p$越大越好\n",
    "    + 当类标签为0时，希望样本为$1$的概率$1-p$越大越好\n",
    "对于一个批次的样本，可以得到优化目标：\n",
    "$$Loss = \\prod_{i=1}^{N} {p(y_{i}|x_{i},\\theta)}$$\n",
    "添加log等效转化：\n",
    "$$Loss = \\sum_{i=1}^{N} {{log}{\\; p(y_{i}|x_{i},\\theta)}}$$\n",
    "$$Loss = \\sum_{i=1}^{N} {{log}{\\; p^{y}(1-p)^{1-y}}}$$\n",
    "$$Loss = \\sum_{i=1}^{N} [{{p*log}{\\; y}+(1-p){log}{ \\;(1-y)}}]$$\n",
    "一般情况写可以转换成均值,并转化为最小化问题：\n",
    "$$Loss = - \\frac{1}{N} \\sum_{i=1}^{N} [{{y*log}{\\; p}+(1-y){log}{ \\;(1-p)}}]$$\n",
    "公式化展示：\n",
    "$$\\theta =  arg\\underset{\\theta}{min} {\\; Loss}$$\n",
    "3. <font color=red size=2.8>BCELoss里面的权重$w$方法暂时不知道有什么用 </font>\n",
    "\n",
    "```python\n",
    "# 代码\n",
    "loss = nn.BCELoss(reduction='sum')\n",
    "x = torch.ones(1,10)\n",
    "y = torch.zeros(1,10)\n",
    "\n",
    "x = torch.sigmoid(x)\n",
    "y = torch.sigmoid(y)\n",
    "a = loss(x,y)\n",
    "print(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.BCEWithLogitsLoss\n",
    "1. 和 nn.BCELoss 功能相同，用于二分类但是加上了sigmod层，比直接加更加稳定。\n",
    "$$\\text{loss}(\\mathbf{x}_i, \\mathbf{y}_i) = - \\boldsymbol{w}_i \\left[{y}_i \\log \\mathbf \\sigma{({x}_i)} + (1-{y}_i)\\log(1-\\sigma{({x}_i)}) \\right ]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss \n",
    "1. 用于多分类,注意该损失函数并不对称，也就是说prediction和label的位置不弄混。\n",
    "2. 交叉熵损失函数包括三个部分\n",
    "    + 第一部分：标签$y$的$ont-shot$编码\n",
    "    + 第二部分：预测值$\\hat{y}$的$softmax$转换\n",
    "    + 第三部分：交叉熵的计算\n",
    "3. 用公式表示\n",
    "$$y = one\\_hot(y)$$\n",
    "$$\\hat y = softmax(y) = \\frac{ e^{x_{i}}}{\\sum^{i=1}_{i=N}e^{x_{i}}}$$\n",
    "$$loss(\\hat{y},y) = -log {\\; \\hat y \\cdot y}$$\n",
    "4. <font color=red size=2.8>在Pytorch中使用是经常会出现一些Long和float的问题</font>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "# from sklean.preprocessing import OneHotEncoder\n",
    "def CrossEntropy(prediction,target):\n",
    "    prediction = prediction.reshape(1,len(prediction))\n",
    "    target = np.transpose(target.reshape(1,len(target)))\n",
    "    \n",
    "    log_label = np.log(prediction)\n",
    "    \n",
    "    data = -np.matmul(log_label,target)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def log_softmax(input):\n",
    "    exp_input = np.exp(input)\n",
    "    exp_input = np.log(np.array([val / sum(exp_input) for val in exp_input]))\n",
    "    return exp_input\n",
    "\n",
    "CrossEntropy(softmax(np.array([0.2,0.6,0.2])),np.array([1,0,0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.NLLLoss\n",
    "1. 多分类\n",
    "2. 等价于 CrossEntropy = log_softmax + NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "def log_softmax(input):\n",
    "    exp_input = np.exp(input)\n",
    "    exp_input = np.log(np.array([val / sum(exp_input) for val in exp_input]))\n",
    "    return exp_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4794,  3.2893, -0.6448,  0.5649, -0.3041]], requires_grad=True)\n",
      "tensor([[-3.8960, -0.1273, -4.0614, -2.8517, -3.7207]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "tensor([[-0.4794,  3.2893, -0.6448,  0.5649, -0.3041]], requires_grad=True)\n",
      "[-0.47941372  3.2893145  -0.644789    0.56485087 -0.30408052]\n",
      "[-3.8960045  -0.12727631 -4.06137983 -2.85173991 -3.72067135]\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,5,requires_grad=True)\n",
    "target = torch.tensor([1,0,0])\n",
    "# output = loss(torch.tensor(log_softmax()),target)\n",
    "print(input)\n",
    "print(m(input))\n",
    "print()\n",
    "print()\n",
    "print(input)\n",
    "input = input.detach().numpy()\n",
    "print(input[0])\n",
    "print(log_softmax(input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red size=20>nn.KLDivLoss</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
