{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "1. feature scaling\n",
    "    + $w_{1}$和$w_{2}$的大小范围不同，比如$w_{1}$的大小范围属于$[1,10]$,比如$w_{2}$的大小范围属于$[1,100]$,一般采用标准化进行处理\n",
    "    + 在深度学习中，可以想象，如果feature scaling对于单层的输入有很好的效果，那么在多层神经网络中，同样的操作是不是会取得更好的结果，因此可以在每层网络输入之前都进行batch normal的处理。缓解了internal covariate shift\n",
    "2. batch  normalization\n",
    "    + 每次都会计算一个batch: $\\frac{z^{i} - \\mu }{\\sigma}$,在反向传播的时候，反向传播的路径还是会经过$\\mu$和$\\sigma$\n",
    "        + $ z^{i} = \\frac{z^{i} - \\mu }{\\sigma}$\n",
    "        + $ z^{i} = \\gamma \\bigodot z^{i} + \\beta $\n",
    "3. 使得输入的数据在深度神经网路传播的过程中更加稳定\n",
    "4. nn.BatchNorm2d(num_feature),其中num_feature表示的是batch_size * num_feature * width * size,通道数channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 32\n",
    "num_channel = 3\n",
    "num_z = 100 # number of noise input\n",
    "number_epoch = 10\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_loader = data.DataLoader(\n",
    "    dataset=datasets.CIFAR10(root='./data',download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.Resize(image_size),\n",
    "                                 transforms.CenterCrop(image_size),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                             ])),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 反卷积\n",
    "    + torch.nn.ConvTranspose2d(input=100, output=8, kernel_size = (4, 4), stride=1, padding=0)\n",
    "2. 数学表达\n",
    "    + 正常的卷积操作，比如有一个输入是$A_{4\\times 4}$的矩阵：卷积核的大小为3，对数据进行展开转换成矩阵的乘法：\n",
    "    $$Y_{4*1} = C_{4*16} \\times X_{16*1}$$\n",
    "    + 反卷积操作：\n",
    "    $$X_{16*1} =C^{T}_{4*16} \\times Y_{4*1}$$\n",
    "    + Hint: 记住只有方阵才可逆，这里并不要求卷积核和反卷积核相同，只是反应一种矩阵操作。\n",
    "3. 反卷积的尺寸大小计算\n",
    "    + 定义一些参数：$H_{output}$表示输出图片的高度，$H_{input}$表示输入图片的高度，$H_{kernel}$表示卷积核的大小，最后得到的计算公式：\n",
    "    $$H_{output} = (H_{input} - 1) * stride + H_{kernel} - 2 *padding$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution, init 32 * 32\n",
    "            nn.ConvTranspose2d(100, 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # state size.  (1 - 1) * 1 + 4 - 2*0=  4, 8 * 4 * 4\n",
    "            nn.ConvTranspose2d( 8, 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            # state size.  (4 - 1) * 2 + 4 - 2 * 1 = 8 \n",
    "            nn.ConvTranspose2d(4, 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (8 - 1) * 2 + 4 - 2 * 1 = 16 \n",
    "            nn.ConvTranspose2d(2, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() # (16 - 1) * 2 + 4 - 2 * 1 = 32\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        insize = x.size(0)\n",
    "        output = self.main(x)\n",
    "        return output.view(insize,3,32,32)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3,10,kernel_size=4,stride=2,padding=1), # (32 - 4 + 2*1) / 2 + 1 = 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(10,15,kernel_size=4,stride=2,padding=1),# (16 - 4 + 2*1) / 2 + 1 = 8\n",
    "            nn.BatchNorm2d(15),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(15,5,kernel_size=4,stride=2,padding=1),# (8 - 4 + 2*1) / 2 + 1 = 4\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(5,1,kernel_size=4),#  4 - 4 + 1 = 1\n",
    "            nn.Sigmoid() # 输出依旧是概率数值\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        insize = x.size(0)\n",
    "        output = self.main(x).view(insize,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "dis = Discriminator()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optim_G = torch.optim.Adam(gen.parameters())\n",
    "optim_D = torch.optim.Adam(dis.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么是 先更新判别器 后更新生成器 ？？？balck man's mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-0a680e1c9ab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moptim_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss_d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptim_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss_g_sum = 0.0\n",
    "    loss_d_sum = 0.0\n",
    "    for step,(data,label) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        size = data.size(0)\n",
    "        z = torch.randn(size,100,1,1).to(device)\n",
    "        \n",
    "        ones_label = torch.ones(size,1).to(device)\n",
    "        zeros_label = torch.zeros(size,1).to(device)\n",
    "        \n",
    "        \"\"\"\n",
    "        固定生成器G，更新判别器D\n",
    "        \"\"\"\n",
    "        print(data.size())\n",
    "        d_real = dis(data)\n",
    "        d_fake = dis(gen(z))\n",
    "        \n",
    "        loss_real = criterion(d_real,ones_label)\n",
    "        loss_fake = criterion(d_fake,zeros_label)\n",
    "        \n",
    "        loss_d = loss_real + loss_fake\n",
    "        \n",
    "        optim_D.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        z = torch.randn(size,100,1,1).to(device)\n",
    "        loss_g = criterion(dis(gen(z)),ones_label)\n",
    "        \n",
    "        optim_G.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        loss_g_sum += loss_g.item()\n",
    "        loss_d_sum += loss_d.item()\n",
    "    \n",
    "    print(\"Epoch %d G Loss %.3f, D Loss %.3f\" % (epoch, loss_g_sum / step,loss_d_sum / step))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net,self).__init__()\n",
    "        \n",
    "#         self.layer = nn.BatchNorm2d(8)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         output = self.layer(x)\n",
    "#         return output\n",
    "\n",
    "# real_batch = next(iter(data_loader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我现在理解的GANs\n",
    "1. 在第二步，更新生成器的过程中实际上是将$P_{data}$和$P_{G}$之间的分布距离$JS-Divergence$转化成了最小化判别器损失函数的问题,或者说不断调整生成网络的参数，使得生成函数所得到的数据分布$P_{G}$不断接近真实分布$P_{data}$。\n",
    "2. __生成器与判别器不均衡的问题，在编码的过程中常常会出现一些问题，就是判别器训练的太好，判别器的损失函数很快地就降为0。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
