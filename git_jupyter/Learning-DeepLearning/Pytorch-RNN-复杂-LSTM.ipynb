{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "### 框架\n",
    "Long short-term memory 主要是为了解决RNN<font color='red'>长期依赖</font>的问题,LSTM最重要的概念在与引入了cell state的概念\n",
    "1. 输入门： 决定是否接收当前的输入\n",
    "2. 遗忘门： 决定是否遗忘上一时刻的hidden\n",
    "3. 输出门： 决定是否output当前的输出\n",
    "<img src=\"./image/LSTM.png\" style=\"zoom:40%\">\n",
    "\n",
    "### 定义\n",
    "为了更好地说明LSTM的计算过程，首先定义一些符号：<br>\n",
    "$x_{t}表示当前t时刻的输入$ <br>\n",
    "$W_{i},W_{f},W_{o}表示的是三个门的权重矩阵，和x_{t}进行运算$ <br>\n",
    "$W_{c}表示的是cell的权重矩阵，和x_{t}进行运算$<br>\n",
    "$U_{i},U_{f},U_{o}表示的是三个门的权重矩阵，和h_{t-1}进行运算$ <br>\n",
    "$U_{c}表示的是cell的权重矩阵，和h_{t-1}进行运算$<br>\n",
    "$V_{o}表示的是输出门的权重矩阵，和c_{t}进行运算$<br>\n",
    "\n",
    "\n",
    "### 计算过程\n",
    "#### 输入门 input gate\n",
    "输入门的状态有两部分确定，第一部分是当前的输入 $x_{t}$，第二部分是上次输出的 $h_{t-1}$。<br>\n",
    "候选的memory cell也由上面的两部分确定,候选的原因在于输入门的控制使得memory cell不一定能够输入。\n",
    "$$输入门的状态：i_t = \\sigma (W_ix_t+U_{i}h_{t-1}+b_{i})$$\n",
    "$$候选的memory\\;cell：\\widetilde{C}_t = tanh(W_cx_t+U_ch_{t-1}+b_{c}) $$\n",
    "\n",
    "#### 遗忘门 forget gate\n",
    "遗忘门也由两部分确定，第一部分是当前的输入 $x_{t}$，第二部分是上次输出的 $h_{t-1}$。<br>\n",
    "$$f_t = \\sigma (W_fx_t+U_fh_{t-1}+b_f)$$\n",
    "\n",
    "#### 计算memory cell ，这里的计算时乘法，<font color = 'red'> 也就是元素乘以对应的元素 </font>\n",
    "在拥有了输入门和遗忘门，以及候选的memory cell之后，可以决定当前的memory cell的数值：\n",
    "当前的memory cell由四个部分组成：输入门，遗忘门，当前输入构成的候选memory cell，上一时刻的memory cell\n",
    "$$C_t = i_t*\\widetilde{C}_t + f_t * C_{t-1}$$\n",
    "\n",
    "\n",
    "#### 计算输出门\n",
    "输出门是由三个部分组成：1. 当前的输入 $x_{t}$  2. 上一次输入的 $h_{t-1}$ 3. 当前的memory cell $C_t$的状态<br>\n",
    "而新的隐藏层则由输出门和当前的memory cell $C_t$决定 <br>\n",
    "$$o_t = \\sigma(W_ox_t+U_oh_{t-1}+ V_oC_t + b_o)$$\n",
    "$$h_t = o_t * tanh(C_t)$$\n",
    "\n",
    "#### 门的操作\n",
    "这是一个连续的操作<br>\n",
    "The sigmoid layer outputs numbers between zero and one, describing <font color='red'>how much of each component</font> should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open(\"input.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 chars, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data),len(chars)\n",
    "print(\"data has %d chars, %d unique\" % (data_size,X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_size = 100\n",
    "learning_rate = 1e-1\n",
    "\n",
    "T_steps = 25\n",
    "weight_sd = 0.1\n",
    "z_size = H_size + X_size # 表示将上次的hidden与当前的输入结合在一起的size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param\n",
    "## 输入\n",
    "$X_t$ : 当前的输入 $X_t$ 是一个包含n个特征的一行数据，它的特征的大小可以表示为 $X_{1\\times n}$<br>\n",
    "$h_{t-1}$:上一时刻的隐藏层$h_{t-1}$的大小取决与隐藏层设置的大小，比如设置为$m$，那么$h_{t-1}$的大小可以表示为$h_{1\\times m}$<br>\n",
    "$Z_t$: $X_t$ 和 $h_{t-1}$ 合并得到的结果，大小可以表示为 $Z_{1\\times (m+n)}$ <br>\n",
    "$C_{t-1}$: 记忆单元的尺寸，和$h_{t-1}$尺寸相同，$C_{1 \\times m}$\n",
    "\n",
    "## 矩阵\n",
    "$W_f$: 遗忘门矩阵，大小可以表示为 $W_{m \\times (m+n)}$\n",
    "$W_i$: 输入门矩阵，大小可以表示为 $W_{m \\times (m+n)}$\n",
    "$W_o$: 输出门矩阵，大小可以表示为 $W_{m \\times (m+n)}$\n",
    "$W_f$: Memory cell 运算矩阵，大小可以表示为 $W_{m \\times (m+n)}$\n",
    "\n",
    "## 输出\n",
    "$W_v$: 输出运算矩阵，假设输出$q$个类，$h_t$的大小为m,那么$W$的矩阵可以表示为$W_{m \\times q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self,name,value):\n",
    "        self.name = name\n",
    "        self.v = value\n",
    "        self.d = np.zeros_like(value) # 导数\n",
    "        self.m = np.zeros_like(value) # 动量 for AdaGard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # 遗忘门\n",
    "        self.W_f = Param('W_f',np.random.randn(H_size,z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',np.zeros((H_size,1)))\n",
    "        \n",
    "        # 输入门\n",
    "        self.W_i = Param('W_i',np.random.randn(H_size,z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',np.zeros((H_size,1)))\n",
    "\n",
    "        # 输出门\n",
    "        self.W_o = Param('W_o',np.random.randn(H_size,z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',np.zeros((H_size,1)))\n",
    "        \n",
    "        self.W_C = Param('W_C',np.random.randn(H_size,z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',np.zeros((H_size,1)))\n",
    "        \n",
    "        self.W_v = Param('W_v',np.random.randn(X_size,H_size) *  weight_sd)\n",
    "        self.b_v = Param('b_v',np.zeros((X_size,1)))\n",
    "    \n",
    "    def all(self):\n",
    "        return [self.W_f,self.W_i,self.W_C,self.W_o,self.W_v,\n",
    "               self.b_f,self.b_i,self.b_C,self.b_o,self.b_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params  = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x,h_prev,C_prev,p=params):\n",
    "    \"\"\"\n",
    "    x,h_prev,C_prev依次表示当前的输入，上一时刻隐藏元，上一时刻内存单元\n",
    "    x.shape = x_size,1\n",
    "    h_prev = H_size,1\n",
    "    C_prev = H_size,1\n",
    "    \"\"\"\n",
    "    \n",
    "    z = np.row_stack((h_prev,x)) # (x_size + H_size = z_size) * 1\n",
    "    \n",
    "    f = sigmoid(np.dot(p.W_f,z) + p.b_f)\n",
    "    i = sigmoid(np.dot(p.W_i,z) + p.b_f)\n",
    "    o = sigmoid(np.dot(p.W_o,z) + p.b_f)\n",
    "    \n",
    "    C_bar = tanh(np.dot(p.W_C,z) + p.b_c)\n",
    "    C = i * C_bar + f * C_prev\n",
    "    \n",
    "    h_t = o * tanh(C)\n",
    "    \n",
    "    v = np.dot(p.W_v,h_t) + p.b_v\n",
    "    \n",
    "    \n",
    "    y = np.exp(v) / np.exp(v).sum()\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播 backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 损失函数分为两个部分：第一部分时在$t$时刻产生的输出$y_t$造成的误差，第二部分时输出的每个类$\\hat{y}_j$与真实结果之间的误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(target,dh_next,dC_next,C_prev,):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
